# Task Tracker

Проект для управления задачами с клиент-серверной архитектурой.

## Описание проекта

Task Tracker - это веб-приложение для создания, отслеживания и управления задачами. Проект состоит из:

- **Клиентской части** - Next.js приложение с React и (MobX/Redux) для управления состоянием. Store можно переключить перейдя на ветку MobX/redux соответственно
- **Серверной части** - Express сервер с JSON Server для работы с данными

## Установка и запуск

### Предварительные требования

- Node.js (версия 20.17.0 или выше)

### Установка зависимостей

```
npm install
```

### Запуск проекта

```
npm run dev
```

### Сборка проекта

```
npm run build
```

### Запуск в production режиме

```
npm run start
```

### Проверка кода

```
npm run lint
```

### Просмотр размер bundle

```
npm run build:analyze
```

# Сравнение Redux и MobX

## Детальное сравнение

| Критерий                        | Redux                                                                                                                                           | MobX                                                                                                                                               |
| ------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- |
| **1. Философия / Паттерн**      | **Функциональное программирование (FP)**.<br>Состояние **иммутабельно**. Изменения через чистые функции.                                        | **Реактивное и ООП-программирование**.<br>Состояние **мутабельно**. Использует наблюдаемые объекты.                                                |
| **2. Поток данных**             | **Однонаправленный, явный.**<br>`View → Action → Reducer → Store → View`                                                                        | **Двунаправленный, автоматический.**<br>`View → Action (мутация State) → (автоматически) View`                                                     |
| **3. Как изменяется состояние** | Через **"action"** (объект) и **"reducer"** (чистая функция).<br>Reducer возвращает **НОВУЮ** копию состояния.                                  | Прямое присваивание (**мутация**) в **"action"**.<br>Просто меняем свойство объекта.                                                               |
| **4. Структура состояния**      | Единое, **нормализованное** состояние в виде plain JS-объекта. Часто неизменяемое (с помощью Immer).                                            | **5. Денормализованное**, может состоять из множества взаимосвязанных **"store"**-классов.                                                         |
| **6. Подписка на изменения**    | Компонент явно подписывается на части состояния с помощью **`useSelector`**. Перерисовка происходит, когда ссылка на эту часть меняется.        | Компонент автоматически отслеживает (**track**) используемые **observable**-поля. Перерисовка при их изменении.                                    |
| **7. Сложность кода**           | **Больше шаблонного кода (boilerplate).** Нужно создавать константы, action creators, reducers.                                                 | **Меньше кода.** Почему бы не изменить состояние напрямую? `this.todos.push(newTodo)`                                                              |
| **8. Кривая обучения**          | Выше. Нужно понимать иммутабельность, чистые функции, иногда middleware (redux-thunk, redux-saga).                                              | Ниже. Более интуитивно для тех, кто привык к ООП и прямому изменению данных.                                                                       |
| **9. Производительность**       | Хорошая. Компоненты перерисовываются, только когда их выделенные данные изменились по ссылке.                                                   | Отличная. Система автоматически отслеживает зависимости и обновляет только те компоненты, которые "видят" самые мелкие изменения.                  |
| **10 .Отладка**                 | **Превосходная.** DevTools позволяют "путешествовать во времени", смотреть каждое действие и состояние. Предсказуемость помогает находить баги. | Хорошая, но другая. MobX имеет DevTools, но из-за мутаций и автоматических реакций иногда сложнее проследить цепочку причинно-следственных связей. |
| **11. Масштабируемость**        | Очень высокая. Строгая структура и предсказуемость помогают в больших командах и на сложных проектах.                                           | Хорошая, но требует большей дисциплины от разработчиков, чтобы не создать "магию" и неразбериху с зависимостями.                                   |

### Паттерн, Поток, Как изменяется состояние

В проекте расставленны `console.log()` для отслежавание потоков store, для того что бы увидеть `console.log()`, требуется открыть вкладку в режиме инкогнито.

1.  При сооздании задачи. Код начнет выполнятся Сверху вниз выполняя каждую функциюю не откладывая ее из-за чего чтение станет "Явным" и в console мы увидим приходящии console по порядку:
    ```javascript
    console.log('1 до строчки выполнения action:', tasksByStatus);
    console.log('2 после строчки выполнения action:', tasksByStatus);
    ```
    Тут стоит обратить внимение Redux, не изменит состояние сразу т.к. работает с копией объекта и имеет **иммутабельное** состояние, состоянеи обновится только на следующий рендер компонента, когда придет новый объект

У MobX "Двунаправленный, автоматический" поток и если рассмотреть его выполнение, то мы увидим, как состояние меняется сразу за счет **мутабельности**.

### Время создания задач.

В коде по ключевому слову "COMMENT" можно найти закомментированый код который покажет скорость выполнения создание 1000 задач

- redux - 370.39999997615814
- mobx - 21.599999994039536
  Итог MobX - примерно в 17 раз быстрее чем redux!!!

### Выделение памяти на создание задач.

В коде по ключевому слову "COMMENT" можно найти закомментированый код (checkMemory) который покажет сколько требуется памяти для создание заадач

- redux - Было занято памяти: 21.21 MB
- mobx - Было занято памяти: 64.71 MB
  Итог redux - примерно в 3 раз меньше занимает памяти!!!

# "@next/bundle-analyzer"

"@next/bundle-analyzer" - это официальный пакет от Next.js для анализа размера сборки вашего приложения.

`npm run build:analyze - запускает сборки приложение и открывает 2 вкладки серверного и клиентского bundle`

В поиске "Filter to initial chunks:" можно выбрать путь файла, что бы более детальней посмотреть бандл.
Это позволит находит большие бандлы и разбивать их.
Так же можно переносить bundle c серверного и клиентского использую `const Component = dynamic(() => import('...'));`

# "WebWorker"

## Где используем

- **Массовое создание задач.** При клике `+ 100 задач` запускается воркер `public/createTasks.worker.js`. Он генерирует 100 задач с искусственной задержкой, отдает прогресс и по завершении возвращает готовый массив, который одним действием добавляется в Redux.
- **Подготовка данных к рендеру.** Воркер `public/renderTasks.worker.js` принимает список задач колонки, фильтрует/подготавливает данные и сообщает, когда можно безопасно обновлять UI.

## Правила подключение Worker в Next проекты

1. **Хранение в `public/`.** Next.js обслуживает воркеры как статические файлы. Каждый воркер обязан лежать в `public` и иметь расширение `.worker.js`, иначе `new Worker('/name.worker.js')` не сработает.
2. **Только чистый JS.** Внутри воркера нельзя использовать TypeScript, алиасы `@/`, import/export. Всё определяется прямо в файле (константы, enum-ы, типы в виде JSDoc при необходимости).
3. **Создание из клиента.** В React-коде инициализируем воркер строго на клиенте (`useEffect` / проверка `typeof window !== 'undefined'`) и вызываем `new Worker('/createTasks.worker.js')`.
4. **Настройка `next.config.js`.** Для корректной работы воркеров в браузере необходимо установить `config.output.globalObject = 'self'` внутри кастомного webpack-конфига (иначе бандл может пытаться обратиться к `window` и упасть).
5. **Контроль жизненного цикла.** После использования обязательно `worker.terminate()` (например, в cleanup-функции `useEffect`). Иначе воркеры продолжают работать в фоне.
6. **Обмен сообщениями.** Придерживаемся единого протокола: `postMessage({ type, payload })`, перечисление действий (`WORKER_ACTION`) и разделение входящих/исходящих сообщений на уровне типов.
7. **Логирование и ошибки.** Желательно писать лог внутри воркера и в месте, где он создаётся, чтобы проще отлавливать ошибки (`worker.onerror`, `worker.onmessageerror`).

## Плюсы

- **Параллельность без потоков.** Позволяют выполнять CPU-интенсивные задачи параллельно с основным UI-потоком без блокировок (парсинг, вычисления, работа с большими массивами, подготовка рендер-данных).
- **Предсказуемый UX.** Воркер можно превращать в «движок» бизнес-логики: UI получает только события прогресса/результата, поэтому легче строить спиннеры, прогресс-бары, пошаговые сценарии.
- **Безопасная песочница.** Код воркера изолирован: нет доступа к DOM, глобальному состоянию окна, что упрощает защиту от случайных побочных эффектов и гонок.
- **Масштабируемость.** Можно запускать несколько воркеров (или WorkerPool) и распределять задачи между ними, приближаясь к многопоточности в браузере.

## Минусы и ограничения

- **Нет прямого доступа к DOM / Window / localStorage.** Любые данные нужно сериализовать и передавать через `postMessage`, что добавляет накладные расходы.
- **Обязательная сериализация.** Передача крупных структур (ArrayBuffer, ImageBitmap) требует либо копирования, либо аккуратного использования Transferable, иначе теряем производительность.
- **Дополнительная инфраструктура.** Нужно продумывать протокол сообщений, таймауты, обработку ошибок, terminate/cleanup; без этого легко получить висящие воркеры или «тихие» ошибки.
- **Ограничения окружения.** В Next.js, Vite и др. есть свои правила подключения (`public/*.worker.js`, `new URL(..., import.meta.url)` и т.д.), что усложняет настройку.
- **Дебаг/сборка.** Sourcemaps для воркеров нередко ломаются. Логирование крайне не удобны для дебага.
- **Не подходит для мелких задач.** Запуск воркера имеет цену (инициализация, передача данных), поэтому простые операции быстрее выполнить напрямую в UI-потоке.

# Redux DevTools

Redux DevTools — это инструмент разработчика для отладки состояния приложения в реальном времени.
у Redux DevTools, есть 2 способа использование это установить пакет в проект и скачать разрешенеи в chrome.
Лично я использовал 2 способ, что намного удобней и быстрей.

### Вкладка "Inspector"

    Основная панель для мониторинга действий и состояния. На ней можно можно видить все действия что происходит в redux

### Вкладка "State"

    Показывает состояние всего redux, в качестве object, где можно увидит, хранимое состояние.

### Вкладка "Action"

    Там видно какой Action срабатывает и что в него переданно

### Вкладка "Diff"

    Показывает изменения состояния, состояние которое стало после выполнения action

### Commit

    зафиксировать текущее состояние как начальную точку

### Reset

    вернуться к начальному состоянию

### Revert

    отменить последнее действие

### Вкладка "log Monitor"

    Показывает, какие action выполнются и какое состояние после action становится

### Вкладка "Chart"

Иерархия состояния

- Круговые диаграммы размера частей состояния
- Дерево состояния с размерами
- Выявление "раздутых" частей состояния, которые выполняются слишком много раз, или слишком большие

### Вкладка "Dispatcher"

    позволяет в ручную отправить action из расширения

# SEO опитмизация

## Советы от разработчиков Google

### Когда нужен Sitemap (https://developers.google.com/search/docs/crawling-indexing/sitemaps/overview?hl=ru)

#### В каких случаях нужен файл Sitemap:

- У вас крупный сайт. Как правило, на таких сайтах сложно выяснить, ведет ли на каждую страницу по меньшей мере одна ссылка с другой его страницы. Это может привести к тому, что робот Googlebot пропустит некоторые недавно добавленные страницы.
- Сайт создан недавно, и на него ведет мало внешних ссылок. Робот Googlebot и другие поисковые роботы сканируют сайты, переходя к URL, обнаруженным на уже обработанных страницах. Если на ваш сайт нет ссылок с других ресурсов, Googlebot может и не заметить такие страницы.
- Сайт содержит большой объем мультимедийного контента (видео и изображений) или представлен в Google Новостях. Из файлов Sitemap поисковая система может получать дополнительную информацию для показа в результатах поиска.

#### В каких случаях файл Sitemap не нужен:

- Сайт сравнительно невелик. Иными словами, на нем не больше 500 страниц, которые должны быть представлены в результатах поиска.
- На сайте реализована детальная система внутренних ссылок. Это означает, что роботы Google могут перейти во все значимые разделы сайта, следуя по ссылкам с главной страницы.
- На сайте сравнительно мало медиафайлов (видео и изображений) или новостных страниц, которые вы хотите показывать в результатах поиска. Файлы Sitemap помогают роботам Google находить и анализировать видео, изображения и новости с сайтов. Если вам не нужно, чтобы такой контент появлялся в результатах поиска Google, то файл Sitemap вам не потребуется.

### Rodots.txt (https://developers.google.com/search/docs/crawling-indexing/robots/intro?hl=ru)

robots.txt — это первая остановка для робота. Он смотрит сюда, чтобы понять, куда ему можно идти.

Например:

```
User-agent: \*
Disallow: /admin/
Disallow: /tmp/
Allow: /

```

Такой код можно будет перевести. "Всем роботам: не заходи в папки /admin/ и /tmp/, всё остальное можно"

#### Важно уточнить:

robot.txt

1. НЕ защищает контент — файл публично доступен

2. НЕ скрывает страницы из поиска — для этого нужны другие методы

3. НЕ блокирует доступ пользователей

( Если поисковой робот от google не провалится по путям. Это не значит что поисковой злоумышленников поступит также )

### Мета-теги (https://developers.google.com/search/docs/crawling-indexing/special-tags?hl=ru)

Теги meta – это HTML-код, предназначенный для предоставления поисковым системам и другим клиентам дополнительной информации о веб-странице.

```

<Head>
  {/* Базовые мета-теги для SEO */}
  <title>Название страницы | Сайт</title>
  <meta name="description" content="Краткое описание страницы 150-160 символов" />
  <meta name="keywords" content="ключевые, слова, через, запятую" />

{/_ Open Graph для соцсетей _/}

  <meta property="og:title" content="Заголовок для соцсетей" />
  <meta property="og:description" content="Описание для соцсетей" />
  <meta property="og:image" content="/social-preview.jpg" />
  <meta property="og:url" content="https://site.com/page" />

{/_ Каноническая ссылка против дублей _/}

  <link rel="canonical" href="https://site.com/page" />
</Head>
```

#### Общий принцип работы поискового робота

![Схема](./image.png)

1. Скачивание html

Он скачивает HTML-код страниц, сохраняя сырой код.

2. Парсинг html

Скачав код, робот анализирует его структуру:

Извлекает текст из тегов (\<h1>, \<p>, \<a>)

Читает мета-теги (\<title>, description, canonical)

Находит все ссылки на другие страницы

Выявляет основной контент, отфильтровывая шапку, меню, футер (семантические тэги)

3. Индексация (отдельный алгоритм)

Полученную информацию робот систематизирует и заносит в гигантскую базу данных (индекс):

Разбивает текст на слова и фразы

Анализирует частоту и значимость слов

Запоминает связи между страницами через ссылки

Сохраняет служебную информацию (дату обновления, автора и т.д.)

4. Ранжирование

Когда пользователь вводит запрос, поисковик не бежит по всему интернету, а ищет в своем индексе:

Сравнивает запрос с проиндексированными страницами

Оценивает релевантность (соответствие) каждой страницы запросу

Сортирует результаты по сотням факторов (качество контента, свежесть, авторитетность и т.д.)

5. Постоянное обновление
   Робот регулярно возвращается на страницы:

Проверяет, не изменился ли контент

Обновляет информацию в индексе

Открывает новые ссылки

#### Разбор некоторых тэгов более детальней

1. \<title>Название страницы | Сайт\</title>:

Что делает робот: Видит тег \<title>, понимает, что текст внутри него — это главный заголовок страницы.

Как использует: Это один из самых важных факторов для SEO. Это заголовок, который отображается в результатах поиска (SERP). Робот использует его, чтобы понять основную тему страницы и сопоставить с запросами пользователей.

2. \<meta name="description" content="Краткое описание страницы..."/>

Что делает робот: Видит мета-тег с атрибутом name="description" и считывает текст из атрибута content.

Как использует: Не является прямым фактором ранжирования. Однако робот часто использует это описание как текст сниппета в результатах поиска. Если описание релевантно запросу пользователя, повышается вероятность клика (CTR), что уже влияет на позиции косвенно.

3. Open Graph мета-теги (og:)

Что делает робот: Поисковый робот Google или Яндекс может их учитывать, но в первую очередь они созданы для роботов социальных сетей (Facebook, VK, Telegram, Twitter).

Как использует: Когда вы делитесь ссылкой в соцсети, ее робот находит эти теги и использует их для создания красивого превью (заголовок, описание, картинка).

4. \<link rel="canonical" href="/"/>

Что делает робот: Видит тег \<link> с атрибутом rel="canonical". Это важная служебная инструкция.

Как использует: Робот понимает, что данная страница может быть доступна по разным URL (например, с www и без, с http и https, с параметрами ?utm_source=...). Этот тег говорит ему: "Считай это главной (канонической) версией этой страницы. Все ссылки и вес отдавай этому URL." Это помогает бороться с дублированием контента.

#### Дополнительно

На сайте https://developers.google.com/search/docs/crawling-indexing/special-tags?hl=ru, есть возможно просмотра какие мета тэги не поддерживаются или поддерживаются и для чего они нужны более подробно.
